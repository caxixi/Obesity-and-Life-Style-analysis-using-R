---
title: 'Obesity: demographic and life style: part 1.'
author: "Gal Levin"
date: "5/15/2021"
output:
  html_document:
    toc: yes
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# <span style="color:blue">Why this document</span>

I created this document to:

1. Share some insight and thought about the prevalence of obesity, but also to -
2. Demonstrating data and R skills I acquired: preparation of a dataset, fractioning and querying, visualizations, choosing and performing statistical tests, using loops and custom functions, and walking you, the reader, through all this.


# <span style="color:blue">Introduction</span>

Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System collects data on certain lifestyle aspects and the prevalence of obesity among US residents. Here I analyze the dataset, downloaded from the Center for Disease Control: https://chronicdata.cdc.gov/Nutrition-Physical-Activity-and-Obesity/Nutrition-Physical-Activity-and-Obesity-Behavioral/hn4x-zwk7. The goal is to see how demographic factors and reported aspects of lifestyle are associated with the prevalence of obesity.

Libraries that will be used:


```` {r include = TRUE, warning = FALSE, message = FALSE}

# Calling libraries:
library(dplyr)
options(dplyr.summarise.inform = FALSE)
library(ggplot2)
library(patchwork)
library (car)
library(tidyr)
```


# <span style="color:blue">The data</span> 

Data were collected from states, and, gradually over the years, territories started providing data as well.

For this analysis, every state / territory is given the same weight, regardless of the population size and the sample size. This enables looking for associations across different populations. At times I looked to see if association is maintained across years, but we need not assume that each year reflects a completely different population; some people might have provided data year after year. 

I already made some analysis of an older download of the data. For the crafting of this document, I downloaded a more recent data file from the CDC site. However, it looks like some of the old data was not included in the updated version. Not able to find out why, I joined the new and the old datasets to one.

Here is how the dataset was prepared for this analysis:


```` {r include = TRUE}

# Reading downloaded file:
obesity_old <- read.csv("Obesity.csv", header = T, sep = ",")
obesity_new <- read.csv("Obesity_2021.csv", header = T, sep = ",")

# Selecting columns to analyze:
keep_col<-c("YearEnd","LocationDesc", "Question", "Sample_Size",
            "StratificationCategory1", "Stratification1", "Data_Value")

# Keeping only those columns:
obesity__old_lean <- select(obesity_old, all_of(keep_col))
obesity__new_lean <- select(obesity_new, all_of(keep_col))
```


As mentioned above, it looks like some of the old data were not included in the more recent dataset, namely, the responses to the questions about dietary habits:


```` {r include = TRUE}

anti<-anti_join(obesity_old, obesity_new, by=keep_col)
unique(anti$Question)
```


Not being able to find out why this part of the data was excluded, for the purpose of this analysis the two data sets were appended and duplicated rows removed. Also removed are the national data, as the aim here is to look for associations that are kept across different (US) populations; looking at national data gives higher weight to large state and states that were more successful in collecting data.

```` {r include = TRUE}

obesity_join<-unique(rbind(obesity__old_lean, obesity__new_lean)%>%
                       arrange(YearEnd, LocationDesc, Question, Sample_Size,
                               StratificationCategory1, Stratification1))%>%
  filter(!LocationDesc == "National") 
```


As for missing values: In this case, the values that are missing are on rows lacking both data values and sample size:


```` {r include = TRUE}

identical(is.na(obesity_join$Sample_Size), is.na(obesity_join$Data_Value))
```


Why is that? 

Some clarification is provided by looking at the smallest sample size:


```` {r include = TRUE}

min(obesity_join$Sample_Size, na.rm=T)
```


Looks like samples of less than 50 were deemed too small to represent a demographic subgroup and was not included. Not seeing merit in assigning an estimated value to those missing data, I opted not to include those rows in the analysis.


```` {r include = TRUE}

obesity_final <- obesity_join[complete.cases(obesity_join), ]

# Verifying that no missing data left:
sum(is.na(obesity_final))
```


Let's look at the rows that were removed:


```` {r include = TRUE}

obesity_na<-obesity_join[rowSums(is.na(obesity_join)) > 0, ]
```


What percentage of rows was removed?


```` {r include = TRUE}

nrow(obesity_na)/nrow(obesity_join)*100
```


This is a substantial fraction.

Let's get an idea as to where and when are the missing data likely to come from.

As from where:


```` {r include = TRUE}

miss_location <- sort(table(obesity_na$LocationDesc), decreasing = T)
head(miss_location, 10)
tail(miss_location, 10)
```


Some states indeed provided more missing data than others. As expected, at the top of the lists are states and territories with smaller populations overall.

As from when:


```` {r include = TRUE}

miss_year <- sort(table(obesity_na$YearEnd), decreasing = T)
miss_year
```


There seems to be a trend: with time, more sub-population did not hit the minimum sample size mark. 

But although the missing values are not homogeneously spread in time and place, no year and no location clearly stand out an exception. The quality of the data may not be optimal, but there is no reason to think the data would mislead us.

Lastly, let's look for odd values, if exist, in the columns that were kept. This can be done by looking at the unique values within the columns, or, when the records are numbers and can take endless values, at the head and the tail of the column's frequency table; that is where extreme values as well as values of exceptional formats are likely to be found.


```` {r include = TRUE}

# Year end column:
class(obesity_final$YearEnd)
unique(obesity_final$YearEnd)

# Location column:
class(obesity_final$LocationDesc)
unique(obesity_final$LocationDesc)

# Questions column:
class(obesity_final$Question)
unique(obesity_final$Question)

# Sample size column:
class(obesity_final$Sample_Size)
samp_size<-table(obesity_final$Sample_Size)
head(samp_size)
tail(samp_size)

# Stratification Category column:
class(obesity_final$StratificationCategory1)
unique(obesity_final$StratificationCategory1)

# Stratification column:
class(obesity_final$Stratification1)
unique(obesity_final$Stratification1)

# Data value column:
class(obesity_final$Data_Value)
dat_val<-table(obesity_final$Data_Value)
head(dat_val)
tail(dat_val)
```


All looks reasonable.

So, let's see what the data are telling us. Please continue to part 2.

